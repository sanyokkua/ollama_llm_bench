### **Project Specification: Ollama LLM Benchmarker**

#### **1. Project Vision & Core Principles**

**Vision:** To provide a user-friendly, desktop application for developers, researchers, and AI enthusiasts to systematically benchmark local Large Language Models (LLMs) running on an Ollama instance. The application will evaluate models on a diverse set of tasks, measuring not only performance (speed, tokens/sec) but also the qualitative accuracy of their responses through an automated, LLM-based judging system.

**Core Principles:**

  * **User-Centric:** The UI must be intuitive, requiring minimal setup. The workflow should be simple: select models, click start, view results.
  * **Responsive:** The UI must never freeze. All long-running operations (benchmarking, judging) must execute in a background thread.
  * **Robust:** The application must handle API errors, timeouts, and unexpected model outputs gracefully. State should be preserved in a local database to allow for recovery or later analysis.
  * **Extensible:** The task dataset must be easy to expand by simply adding new YAML files, without requiring code changes.
  * **Transparent:** The user should have a real-time view of the application's operations, including which task is running and the model's raw output as it's being generated.

-----

#### **2. System Architecture**

The application will follow a Model-View-Controller (MVC) inspired pattern, adapted for PyQt6's signal-slot mechanism and threading model.

  * **View (The UI):** `src/ollama_llm_bench/ui/main_window.py`
      * A single `MainWindow` class (inheriting from `QMainWindow`) responsible for all visual elements. It captures user input and displays data. It does not contain any business logic.
  * **Controller (The Bridge):** Also part of `MainWindow`.
      * It instantiates backend components.
      * It connects UI element signals (e.g., `startButton.clicked`) to the backend controller's slots.
      * It connects signals from the backend (e.g., `progress_update`) to UI update slots (e.g., `update_progress_bar`).
  * **Model (The Backend):** A set of classes responsible for all business logic, running in a dedicated `QThread` to keep the UI responsive.
      * `BenchmarkController`: The orchestrator. Manages the entire benchmark lifecycle.
      * `OllamaManager`: Handles all communication with the Ollama API.
      * `DataManager`: Manages all database read/write operations.
      * `DatasetLoader`: Responsible for loading benchmark tasks from YAML files.

-----

#### **3. Data Models & Schemas**

##### **3.1. YAML Task Definition (`dataset/*.yml`)**

The benchmark tasks are defined in YAML files located within the packaged `dataset` directory. Each file can contain a list of task objects.

**File:** `dataset/coding_tasks.yml`

```yaml
- task_id: "python_fibonacci_iterative"
  category: "Coding"
  sub_category: "Python"
  question: "Write an iterative Python function named `fibonacci` that takes an integer `n` and returns the n-th Fibonacci number. Handle base cases for n=0 and n=1."
  expected_answer:
    most_expected: "A complete, correct, and documented iterative Python function for Fibonacci."
    good_answer: "A correct iterative Python function that works for n > 1 but might miss edge cases or documentation."
    pass_option: "A code snippet that correctly calculates the Fibonacci sequence, even if it's not a perfect function."
  incorrect_direction: "The solution is recursive, uses incorrect logic, has syntax errors, or fails on base cases."
```

##### **3.2. Python Data Classes (`src/ollama_llm_bench/core/data_models.py`)**

These classes represent the data structures used within the application.

```python
from dataclasses import dataclass
from enum import StrEnum

class TaskStatus(StrEnum):
    """Enumeration for the status of a benchmark task result."""
    NEW = "NEW"
    AWAITING_JUDGEMENT = "AWAITING_JUDGEMENT"
    JUDGEMENT_IN_PROGRESS = "JUDGEMENT_IN_PROGRESS" # For finer control
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

@dataclass(frozen=True)
class ExpectedAnswer:
    most_expected: str
    good_answer: str
    pass_option: str

@dataclass(frozen=True)
class BenchmarkTask:
    task_id: str
    category: str
    sub_category: str
    question: str
    expected_answer: ExpectedAnswer
    incorrect_direction: str

@dataclass
class Result:
    """Represents a single task result record in the database."""
    result_id: int | None = None
    run_id: int | None = None
    model_name: str = ""
    task_id: str = ""
    question: str = ""
    llm_response: str = ""
    time_taken_ms: int = 0
    tokens_generated: int = 0
    evaluation_score: float = -1.0 # Use -1.0 to indicate not scored
    evaluation_reason: str = ""
    status: TaskStatus = TaskStatus.NEW
```

##### **3.3. SQLite Database Schema**

The database will be a single file located in the user's application data directory (managed by `platformdirs`).

**Database File Name:** `benchmark_data.db`

**Table 1: `benchmark_runs`**

```sql
CREATE TABLE IF NOT EXISTS benchmark_runs (
    run_id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    judge_model TEXT NOT NULL,
    status TEXT NOT NULL -- e.g., 'RUNNING', 'JUDGING', 'COMPLETED', 'STOPPED'
);
```

**Table 2: `results`**

```sql
CREATE TABLE IF NOT EXISTS results (
    result_id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    model_name TEXT NOT NULL,
    task_id TEXT NOT NULL,
    question TEXT NOT NULL,
    llm_response TEXT,
    time_taken_ms INTEGER,
    tokens_generated INTEGER,
    evaluation_score REAL,
    evaluation_reason TEXT,
    status TEXT NOT NULL, -- Corresponds to the TaskStatus enum
    error_message TEXT, -- To store reasons for failure
    FOREIGN KEY (run_id) REFERENCES benchmark_runs(run_id)
);
```

-----

#### **4. Detailed Component Specifications**

##### **4.1. The User Interface (`src/ollama_llm_bench/ui/main_window.py`)**

**`MainWindow(QMainWindow)`**

  * **Layout:** A `QHBoxLayout` divides the window into a left control panel and a right log view.
  * **Widgets:**
      * **Left Column (`control_panel_layout: QVBoxLayout`):**
          * `judge_label (QLabel)`: Text "Judge Model:".
          * `judge_selector (QComboBox)`: Populated with models from Ollama.
          * `models_label (QLabel)`: Text "Models to Benchmark:".
          * `model_list (QListWidget)`: Populated with models from Ollama. Each item has a checkbox (`Qt.ItemIsUserCheckable`).
          * `refresh_button (QPushButton)`: Text "Refresh Models". Triggers a call to fetch models from Ollama.
          * `start_button (QPushButton)`: Text "Start Benchmark".
          * `stop_button (QPushButton)`: Text "Stop Benchmark". Initially hidden.
          * `progress_bar (QProgressBar)`: Shows overall progress. Format: "%v / %m tasks".
          * `status_label (QLabel)`: Displays the current high-level status (e.g., "Idle", "Running task X/Y", "Judging...", "Completed").
      * **Right Column (`log_panel_layout: QVBoxLayout`):**
          * `log_view (QTextEdit)`: Read-only. Displays a verbose, real-time log of all operations.

##### **4.2. Dataset Loader (`src/ollama_llm_bench/core/dataset_loader.py`)**

  * **Function: `load_tasks() -> list[BenchmarkTask]`**
      * **Logic:**
        1.  Uses `importlib.resources.files('ollama_llm_bench') / 'dataset'` to get a path to the packaged dataset directory.
        2.  Iterates through all `.yml` and `.yaml` files in that directory.
        3.  For each file, opens it, parses the YAML content using `PyYAML`.
        4.  Validates that each entry in the YAML file conforms to the `BenchmarkTask` structure.
        5.  Appends the parsed `BenchmarkTask` objects to a list.
        6.  Returns the flattened list of all tasks from all files.
        7.  Logs errors for malformed files but does not crash.

##### **4.3. Data Manager (`src/ollama_llm_bench/backend/data_manager.py`)**

**`class DataManager`**

  * `__init__()`: Sets up the database connection in the path provided by `platformdirs.user_data_dir()`. Creates tables if they don't exist.
  * `create_run(judge_model: str) -> int`: Inserts a new row into `benchmark_runs` with status 'RUNNING' and returns the `run_id`.
  * `prepare_tasks_for_run(run_id: int, models: list[str], tasks: list[BenchmarkTask]) -> int`:
      * Iterates through each `model` in `models`.
      * Iterates through each `task` in `tasks`.
      * Inserts a new row into the `results` table for each combination with `status='NEW'`, `run_id`, `model_name`, `task_id`, and `question`.
      * Returns the total number of tasks inserted.
  * `fetch_tasks_by_status(run_id: int, status: TaskStatus) -> list[Result]`: Selects all rows from `results` for the given `run_id` and `status`. Returns a list of `Result` objects.
  * `update_task_to_judgement(result_id: int, llm_response: str, time_ms: int, tokens: int)`: Updates a task record with the inference results and sets `status = AWAITING_JUDGEMENT`.
  * `update_task_to_completed(result_id: int, score: float, reason: str)`: Updates a task record with the judge's score and reason, setting `status = COMPLETED`.
  * `update_task_to_failed(result_id: int, error_message: str)`: Updates a task record's `status` to `FAILED` and records the error.
  * `get_run_results_for_report(run_id: int) -> dict[str, list[Result]]`: Fetches all completed/failed results for a run, grouped by `model_name`.

##### **4.4. Ollama Manager (`src/ollama_llm_bench/backend/ollama_manager.py`)**

**`class OllamaManager`**

  * `__init__()`: Initializes `self.client = ollama.Client()`.
  * `list_models() -> list[str]`:
      * Calls `self.client.list()`.
      * Extracts and returns a sorted list of model names (`model['name']`).
      * Handles `ConnectionError` and returns an empty list.
  * `ping_model(model_name: str) -> bool`:
      * Sends a minimal prompt like `ollama.generate(model=model_name, prompt="Respond with a single 'OK'.")`.
      * Returns `True` if a response is received, `False` on timeout or error.
  * `stream_generate(model_name: str, question: str, stream_callback: Callable[[str], None]) -> tuple[str, int]`:
      * Initializes `full_response = ""` and `tokens = 0`.
      * Starts a `time.perf_counter()`.
      * Calls `stream = self.client.generate(model=model_name, prompt=question, stream=True)`.
      * Iterates through the `chunk` in `stream`.
      * For each chunk, appends `chunk['response']` to `full_response` and calls `stream_callback(chunk['response'])`.
      * If `chunk['done']` is `True`, captures `tokens = chunk['eval_count']`.
      * Returns `(full_response, tokens)`.
  * `get_judgement(judge_model: str, task: BenchmarkTask, llm_response: str) -> tuple[float, str]`:
      * **Judge Prompt Template:**
        ```python
        prompt = f"""You are an expert evaluation model. Your task is to score an AI's response against an expected answer.
        Provide your evaluation in a single, minified JSON object with no markdown formatting. The JSON must contain two keys: "score" (a float from 0.0 to 1.0) and "reason" (a brief, one-sentence explanation).

        **Original Task:**
        {task.question}

        **Evaluation Criteria:**
        - **Score 1.0 (Most Expected):** {task.expected_answer.most_expected}
        - **Score 0.7-0.9 (Good):** {task.expected_answer.good_answer}
        - **Score 0.4-0.6 (Passable):** {task.expected_answer.pass_option}
        - **Score < 0.4 (Incorrect):** Fails to meet pass criteria or aligns with: {task.incorrect_direction}

        **AI's Response to Evaluate:**
        ```
        {llm\_response}
        ```

        **Your JSON Output:**
        """
        ```
      * **Logic:**
        1.  Calls `self.client.generate(model=judge_model, prompt=prompt, format='json')`.
        2.  Tries to `json.loads()` the response.
        3.  Validates the loaded JSON has "score" (float) and "reason" (str) keys.
        4.  If parsing or validation fails, retries up to 3 times.
        5.  On success, returns `(score, reason)`.
        6.  On persistent failure, returns `(-1.0, "Judge failed to provide a valid JSON response.")`.

##### **4.5. Benchmark Controller (`src/ollama_llm_bench/backend/benchmark_controller.py`)**

**`class BenchmarkController(QObject)`**

  * **Signals:**

      * `log_message(str)`: For detailed logging to the right-hand text view.
      * `status_update(str)`: For the single status label below the progress bar.
      * `progress_update(int, int)`: `(current_value, max_value)`.
      * `benchmark_finished(str)`: `(markdown_report)`. Emitted on successful completion.
      * `benchmark_stopped()`: Emitted if the user stops the process.
      * `error(str)`: For showing critical error dialogs.

  * **`@pyqtSlot()` Method: `run_full_benchmark()`**

      * This is the main entry point called when the thread starts.
      * **Phase 1: Inference**
        1.  Emit `status_update("Running inference...")`.
        2.  Fetch all tasks with `status='NEW'` for the current run from `DataManager`.
        3.  Group these tasks by `model_name`.
        4.  For each `model`, `model_tasks` in grouped tasks:
            a. Emit `log_message(f"--- Pinging model: {model} ---")`.
            b. If `ollama_manager.ping_model(model)` fails, mark all `model_tasks` as `FAILED` in the DB and continue.
            c. For each `task_result` in `model_tasks`:
            i. Check for stop request flag. If set, break loops.
            ii. Emit `log_message(f"Running task '{task_result.task_id}' on '{model}'...")`.
            iii. Record `start_time`.
            iv. Call `ollama_manager.stream_generate(...)`.
            v. Record `end_time`.
            vi. Update the task in DB to `AWAITING_JUDGEMENT` with results.
            vii. Update progress bar via `progress_update` signal.
      * **Phase 2: Judgement**
        1.  Emit `status_update("Judging responses...")`.
        2.  Fetch all tasks with `status='AWAITING_JUDGEMENT'`.
        3.  Reset progress bar for the judging phase.
        4.  For each `task_result` to be judged:
            a. Check for stop request flag.
            b. Emit `log_message(f"Judging response for '{task_result.task_id}' from '{task_result.model_name}'...")`.
            c. Call `ollama_manager.get_judgement(...)`.
            d. Update task in DB to `COMPLETED` or `FAILED` based on the judge's response.
            e. Update progress bar.
      * **Phase 3: Reporting**
        1.  Emit `status_update("Generating report...")`.
        2.  Call a helper method `_generate_markdown_report()`.
        3.  Emit `benchmark_finished(report_string)`.

  * **Method: `_generate_markdown_report() -> str`**

      * Fetches all final results from `DataManager`.
      * Constructs a markdown string in the specified format.

-----

#### **5. End-to-End Application Flow**

1.  **Launch:** `main.py` starts the `QApplication` and shows `MainWindow`.
2.  **Initialization:** `MainWindow` initializes. It calls a `_refresh_models()` method which uses `OllamaManager` to populate the `judge_selector` and `model_list`. UI controls are in their default state (`start_button` enabled, `stop_button` hidden).
3.  **User Selection:** The user selects a judge model and checks the boxes for models to benchmark. `start_button` becomes enabled only if at least one model and a judge are selected.
4.  **Start Benchmark:** User clicks "Start".
      * `MainWindow` disables the selection controls and the `start_button`. It shows the `stop_button`.
      * It retrieves the selected models.
      * It creates instances of `DataManager`, `OllamaManager`.
      * It calls `data_manager.create_run()` to get a `run_id`.
      * It calls `data_manager.prepare_tasks_for_run()` to populate the DB.
      * It creates `BenchmarkController(managers, run_id, ...)` and a `QThread`.
      * It moves the controller to the thread.
      * It connects the controller's signals to the `MainWindow`'s update slots.
      * It starts the thread, triggering `BenchmarkController.run_full_benchmark()`.
5.  **Execution (Background Thread):**
      * The `BenchmarkController` runs as described in section 4.5.
      * Throughout the process, it emits signals (`log_message`, `status_update`, `progress_update`) which are received by `MainWindow` on the main thread.
      * `MainWindow`'s slots update the UI elements accordingly, ensuring a responsive interface. The `log_view` appends new text, the progress bar updates, etc.
6.  **Completion:**
      * `BenchmarkController` finishes and emits `benchmark_finished` with the markdown report.
      * `MainWindow` receives this signal. It displays the report in a `QMessageBox` or a new dialog.
      * It re-enables the UI controls, ready for another run. The thread is cleaned up.
7.  **Stopping:** If the user clicks "Stop", a flag is set in the `BenchmarkController`. The controller checks this flag between tasks and gracefully exits its loops, updates the run status to 'STOPPED', and emits the `benchmark_stopped` signal.
